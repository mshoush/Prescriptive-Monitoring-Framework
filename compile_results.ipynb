{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\nimport sys\\n\\npd.set_option('display.max_columns', None)\";\n                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\nimport sys\\n\\npd.set_option(\\\"display.max_columns\\\", None)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"from sklearn.preprocessing import OneHotEncoder\\nimport numpy as np\\nimport os\\nimport pandas as pd\\n\\n\\n# standardize column names: [case_id_col, 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = \\\"case_id\\\"\\nactivity_col = 'activity'\\nresource_col = 'resource'\\ntimestamp_col = 'timestamp'\\nlabel_col = 'label'\\ntreatment_col = \\\"Treatment1\\\"\";\n                var nbb_formatted_code = \"from sklearn.preprocessing import OneHotEncoder\\nimport numpy as np\\nimport os\\nimport pandas as pd\\n\\n\\n# standardize column names: [case_id_col, 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = \\\"case_id\\\"\\nactivity_col = \\\"activity\\\"\\nresource_col = \\\"resource\\\"\\ntimestamp_col = \\\"timestamp\\\"\\nlabel_col = \\\"label\\\"\\ntreatment_col = \\\"Treatment1\\\"\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# standardize column names: [case_id_col, 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = \"case_id\"\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data: (235189, 80)\n",
      "\n",
      "Shape of valid data: (339430, 80)\n",
      "\n",
      "Statistics for test data:\n",
      "Number of unique cases: 6283\n",
      "Number of unique activities: 25\n",
      "Number of unique resources: 110\n",
      "Number of unique timestamps: 235177\n",
      "Number of unique labels: 2\n",
      "Number of unique treatments: 2\n",
      "\n",
      "Number of persuadable cases: 3433\n",
      "Number of doNotDistirub cases: 0\n",
      "Number of sureThing cases: 0\n",
      "Number of lostCause cases: 6282\n",
      "\n",
      "Statistics for valid data:\n",
      "Number of unique cases: 9423\n",
      "Number of unique activities: 25\n",
      "Number of unique resources: 118\n",
      "Number of unique timestamps: 339413\n",
      "Number of unique labels: 2\n",
      "Number of unique treatments: 2\n",
      "\n",
      "Number of persuadable cases: 5167\n",
      "Number of doNotDistirub cases: 0\n",
      "Number of sureThing cases: 0\n",
      "Number of lostCause cases: 9421\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"import pandas as pd\\n\\n# Define your column names\\ncase_id_col = 'case_id'\\nactivity_col = 'activity'\\ntimestamp_col = 'timestamp'\\nresource_col = 'resource'\\nlabel_col = 'label'\\n\\ndef custom_encode(value):\\n    if value == '[0]':\\n        return 0\\n    elif value in ('[]', '[1]'): # high uncertainty: empty set or set with positive outcome\\n        return 1\\n    else:\\n        return value\\n\\ndef encode_conformal_data(data):\\n    # Apply the encoding function to columns that start with an alphabetical character\\n    columns_to_encode = [col for col in data.columns if col.startswith(('alpha'))]\\n    data[columns_to_encode] = data[columns_to_encode].applymap(custom_encode)\\n    return data\\n\\ndef assign_causal_class_labels(data):\\n    # assigne causal class labels to each case\\n    # if Proba_if_Treated >= 0.5 and Proba_if_Not_Treated < 0.5 then class = \\\"persuadable\\\" >>> y0: -ve, y1: +ve\\n    # if Proba_if_Treated < 0.5 and Proba_if_Not_Treated >= 0.5 then class = \\\"doNotDistirub\\\" >>> y0: +ve, y1: -ve\\n    # if Proba_if_Treated >= 0.5 and Proba_if_Not_Treated >= 0.5 then class = \\\"sureThing\\\" >>> y0: +ve, y1: +ve\\n    # if Proba_if_Treated < 0.5 and Proba_if_Not_Treated < 0.5 then class = \\\"lostCause\\\" >>> y0: -ve, y1: -ve\\n    data[\\\"class\\\"] = np.nan\\n    data.loc[(data[\\\"Proba_if_Treated\\\"] >= 0.5) & (data[\\\"Proba_if_Untreated\\\"] < 0.5), \\\"class\\\"] = \\\"persuadable\\\"\\n    data.loc[(data[\\\"Proba_if_Treated\\\"] < 0.5) & (data[\\\"Proba_if_Untreated\\\"] >= 0.5), \\\"class\\\"] = \\\"doNotDistirub\\\"\\n    data.loc[(data[\\\"Proba_if_Treated\\\"] >= 0.5) & (data[\\\"Proba_if_Untreated\\\"] >= 0.5), \\\"class\\\"] = \\\"sureThing\\\"\\n    data.loc[(data[\\\"Proba_if_Treated\\\"] < 0.5) & (data[\\\"Proba_if_Untreated\\\"] < 0.5), \\\"class\\\"] = \\\"lostCause\\\"\\n    return data\\n\\n\\n# calculate statistics for each log, i.e., test and valid: number of unique cases, number of unique activities, number of unique resources, number of unique timestamps, number of unique labels, number of unique treatments\\ndef calculate_statistics(data, data_type):\\n    print(f\\\"Statistics for {data_type} data:\\\")\\n    print(f\\\"Number of unique cases: {data[case_id_col].nunique()}\\\")\\n    print(f\\\"Number of unique activities: {data[activity_col].nunique()}\\\")\\n    print(f\\\"Number of unique resources: {data[resource_col].nunique()}\\\")\\n    print(f\\\"Number of unique timestamps: {data[timestamp_col].nunique()}\\\")\\n    print(f\\\"Number of unique labels: {data[label_col].nunique()}\\\")\\n    print(f\\\"Number of unique treatments: {data['Treatment1'].nunique()}\\\\n\\\")\\n    print(f\\\"Number of persuadable cases: {data[data['class'] == 'persuadable'][case_id_col].nunique()}\\\")\\n    print(f\\\"Number of doNotDistirub cases: {data[data['class'] == 'doNotDistirub'][case_id_col].nunique()}\\\")\\n    print(f\\\"Number of sureThing cases: {data[data['class'] == 'sureThing'][case_id_col].nunique()}\\\")\\n    print(f\\\"Number of lostCause cases: {data[data['class'] == 'lostCause'][case_id_col].nunique()}\\\\n\\\")\\n\\n\\n# Function to read and preprocess data\\ndef read_and_preprocess_data(data_type, sample_nr=0, log_name=\\\"bpic2012\\\"):\\n    # Read CSV files\\n    data_csv = pd.read_csv(f\\\"./prepared_data/{log_name}/{data_type}_{log_name}.csv\\\", sep=';')[\\n        [case_id_col, activity_col, timestamp_col, resource_col, label_col, \\\"Treatment1\\\"]\\n    ]\\n\\n    data_encoded = pd.read_csv(f\\\"./prepared_data/{log_name}/{data_type}_encoded_{log_name}.csv\\\", sep=\\\";\\\")\\n    if log_name==\\\"bpic2012\\\":\\n        bpic2012_sample = pd.read_csv(f\\\"./realcause_datasets/bpic17_sample{sample_nr}.csv\\\")\\n    else:\\n        bpic2012_sample = pd.read_csv(f\\\"./realcause_datasets_bpic2017_v2/bpic17_sample{sample_nr}.csv\\\")\\n\\n    \\n    common_columns = data_encoded.columns.intersection(bpic2012_sample.columns)\\n    merged_df = pd.merge(data_encoded, bpic2012_sample, on=list(common_columns)).iloc[:, -5:]\\n\\n\\n    # Read predictive + preds conformal\\n    data_preds_conformal = pd.read_csv(f\\\"./results_from_vm/conformal/{log_name}/conformal_{data_type}_{log_name}.csv\\\", sep=\\\";\\\")\\n\\n    # Read causal + conformal_causal\\n    data_conformal_causal = pd.read_csv(f\\\"./results_from_vm/conformal_causal/{log_name}/conformalizedTE_{log_name}_1_{data_type}.csv\\\", sep=\\\",\\\").iloc[:, -24:]\\n\\n    # Read Survival\\n    data_survival = pd.read_csv(f\\\"./results_from_vm/survival/{log_name}/survival_{data_type}_{log_name}.csv\\\", sep=\\\";\\\").iloc[:, -27:]\\n\\n    data_csv.reset_index(drop=True, inplace=True)\\n    data_preds_conformal.reset_index(drop=True, inplace=True)\\n    data_conformal_causal.reset_index(drop=True, inplace=True)\\n    data_survival.reset_index(drop=True, inplace=True)\\n    merged_df.reset_index(drop=True, inplace=True)\\n\\n    data_all = pd.concat([data_csv, data_preds_conformal, data_conformal_causal, data_survival, merged_df], axis=1)\\n    data_all = data_all.dropna()\\n\\n    # Encode conformal data\\n    data_all = encode_conformal_data(data_all)\\n    data_all = assign_causal_class_labels(data_all)\\n\\n    sorting_cols = [timestamp_col]\\n    data_all = data_all.sort_values(by=sorting_cols).reset_index(drop=True)\\n\\n    # cheack of results_from_vm folder exists\\n    if not os.path.exists(f\\\"./results_from_vm/{log_name}\\\"):\\n        os.makedirs(f\\\"./results_from_vm/{log_name}\\\")\\n\\n    # save data\\n    data_all.to_csv(f\\\"./results_from_vm/{log_name}/{data_type}_{log_name}_all.csv\\\", sep=\\\";\\\", index=False)\\n\\n\\n    # Display head and shape of the data\\n    #print(f\\\"Shape of {data_type} data:\\\")\\n    #print(data_all.shape())\\n    print(f\\\"Shape of {data_type} data: {data_all.shape}\\\\n\\\")\\n\\n    return data_all\\n\\nlog_name = \\\"bpic2017\\\" # \\\"bpic2012\\\"\\nsample_nr = 0\\n# Example usage for test and valid datasets\\ntest_data = read_and_preprocess_data(\\\"test\\\", sample_nr, log_name)\\nvalid_data = read_and_preprocess_data(\\\"valid\\\", sample_nr, log_name)\\n\\n\\ncalculate_statistics(test_data, \\\"test\\\")\\ncalculate_statistics(valid_data, \\\"valid\\\")\";\n                var nbb_formatted_code = \"import pandas as pd\\n\\n# Define your column names\\ncase_id_col = \\\"case_id\\\"\\nactivity_col = \\\"activity\\\"\\ntimestamp_col = \\\"timestamp\\\"\\nresource_col = \\\"resource\\\"\\nlabel_col = \\\"label\\\"\\n\\n\\ndef custom_encode(value):\\n    if value == \\\"[0]\\\":\\n        return 0\\n    elif value in (\\n        \\\"[]\\\",\\n        \\\"[1]\\\",\\n    ):  # high uncertainty: empty set or set with positive outcome\\n        return 1\\n    else:\\n        return value\\n\\n\\ndef encode_conformal_data(data):\\n    # Apply the encoding function to columns that start with an alphabetical character\\n    columns_to_encode = [col for col in data.columns if col.startswith((\\\"alpha\\\"))]\\n    data[columns_to_encode] = data[columns_to_encode].applymap(custom_encode)\\n    return data\\n\\n\\ndef assign_causal_class_labels(data):\\n    # assigne causal class labels to each case\\n    # if Proba_if_Treated >= 0.5 and Proba_if_Not_Treated < 0.5 then class = \\\"persuadable\\\" >>> y0: -ve, y1: +ve\\n    # if Proba_if_Treated < 0.5 and Proba_if_Not_Treated >= 0.5 then class = \\\"doNotDistirub\\\" >>> y0: +ve, y1: -ve\\n    # if Proba_if_Treated >= 0.5 and Proba_if_Not_Treated >= 0.5 then class = \\\"sureThing\\\" >>> y0: +ve, y1: +ve\\n    # if Proba_if_Treated < 0.5 and Proba_if_Not_Treated < 0.5 then class = \\\"lostCause\\\" >>> y0: -ve, y1: -ve\\n    data[\\\"class\\\"] = np.nan\\n    data.loc[\\n        (data[\\\"Proba_if_Treated\\\"] >= 0.5) & (data[\\\"Proba_if_Untreated\\\"] < 0.5), \\\"class\\\"\\n    ] = \\\"persuadable\\\"\\n    data.loc[\\n        (data[\\\"Proba_if_Treated\\\"] < 0.5) & (data[\\\"Proba_if_Untreated\\\"] >= 0.5), \\\"class\\\"\\n    ] = \\\"doNotDistirub\\\"\\n    data.loc[\\n        (data[\\\"Proba_if_Treated\\\"] >= 0.5) & (data[\\\"Proba_if_Untreated\\\"] >= 0.5), \\\"class\\\"\\n    ] = \\\"sureThing\\\"\\n    data.loc[\\n        (data[\\\"Proba_if_Treated\\\"] < 0.5) & (data[\\\"Proba_if_Untreated\\\"] < 0.5), \\\"class\\\"\\n    ] = \\\"lostCause\\\"\\n    return data\\n\\n\\n# calculate statistics for each log, i.e., test and valid: number of unique cases, number of unique activities, number of unique resources, number of unique timestamps, number of unique labels, number of unique treatments\\ndef calculate_statistics(data, data_type):\\n    print(f\\\"Statistics for {data_type} data:\\\")\\n    print(f\\\"Number of unique cases: {data[case_id_col].nunique()}\\\")\\n    print(f\\\"Number of unique activities: {data[activity_col].nunique()}\\\")\\n    print(f\\\"Number of unique resources: {data[resource_col].nunique()}\\\")\\n    print(f\\\"Number of unique timestamps: {data[timestamp_col].nunique()}\\\")\\n    print(f\\\"Number of unique labels: {data[label_col].nunique()}\\\")\\n    print(f\\\"Number of unique treatments: {data['Treatment1'].nunique()}\\\\n\\\")\\n    print(\\n        f\\\"Number of persuadable cases: {data[data['class'] == 'persuadable'][case_id_col].nunique()}\\\"\\n    )\\n    print(\\n        f\\\"Number of doNotDistirub cases: {data[data['class'] == 'doNotDistirub'][case_id_col].nunique()}\\\"\\n    )\\n    print(\\n        f\\\"Number of sureThing cases: {data[data['class'] == 'sureThing'][case_id_col].nunique()}\\\"\\n    )\\n    print(\\n        f\\\"Number of lostCause cases: {data[data['class'] == 'lostCause'][case_id_col].nunique()}\\\\n\\\"\\n    )\\n\\n\\n# Function to read and preprocess data\\ndef read_and_preprocess_data(data_type, sample_nr=0, log_name=\\\"bpic2012\\\"):\\n    # Read CSV files\\n    data_csv = pd.read_csv(\\n        f\\\"./prepared_data/{log_name}/{data_type}_{log_name}.csv\\\", sep=\\\";\\\"\\n    )[[case_id_col, activity_col, timestamp_col, resource_col, label_col, \\\"Treatment1\\\"]]\\n\\n    data_encoded = pd.read_csv(\\n        f\\\"./prepared_data/{log_name}/{data_type}_encoded_{log_name}.csv\\\", sep=\\\";\\\"\\n    )\\n    if log_name == \\\"bpic2012\\\":\\n        bpic2012_sample = pd.read_csv(\\n            f\\\"./realcause_datasets/bpic17_sample{sample_nr}.csv\\\"\\n        )\\n    else:\\n        bpic2012_sample = pd.read_csv(\\n            f\\\"./realcause_datasets_bpic2017_v2/bpic17_sample{sample_nr}.csv\\\"\\n        )\\n\\n    common_columns = data_encoded.columns.intersection(bpic2012_sample.columns)\\n    merged_df = pd.merge(data_encoded, bpic2012_sample, on=list(common_columns)).iloc[\\n        :, -5:\\n    ]\\n\\n    # Read predictive + preds conformal\\n    data_preds_conformal = pd.read_csv(\\n        f\\\"./results_from_vm/conformal/{log_name}/conformal_{data_type}_{log_name}.csv\\\",\\n        sep=\\\";\\\",\\n    )\\n\\n    # Read causal + conformal_causal\\n    data_conformal_causal = pd.read_csv(\\n        f\\\"./results_from_vm/conformal_causal/{log_name}/conformalizedTE_{log_name}_1_{data_type}.csv\\\",\\n        sep=\\\",\\\",\\n    ).iloc[:, -24:]\\n\\n    # Read Survival\\n    data_survival = pd.read_csv(\\n        f\\\"./results_from_vm/survival/{log_name}/survival_{data_type}_{log_name}.csv\\\",\\n        sep=\\\";\\\",\\n    ).iloc[:, -27:]\\n\\n    data_csv.reset_index(drop=True, inplace=True)\\n    data_preds_conformal.reset_index(drop=True, inplace=True)\\n    data_conformal_causal.reset_index(drop=True, inplace=True)\\n    data_survival.reset_index(drop=True, inplace=True)\\n    merged_df.reset_index(drop=True, inplace=True)\\n\\n    data_all = pd.concat(\\n        [\\n            data_csv,\\n            data_preds_conformal,\\n            data_conformal_causal,\\n            data_survival,\\n            merged_df,\\n        ],\\n        axis=1,\\n    )\\n    data_all = data_all.dropna()\\n\\n    # Encode conformal data\\n    data_all = encode_conformal_data(data_all)\\n    data_all = assign_causal_class_labels(data_all)\\n\\n    sorting_cols = [timestamp_col]\\n    data_all = data_all.sort_values(by=sorting_cols).reset_index(drop=True)\\n\\n    # cheack of results_from_vm folder exists\\n    if not os.path.exists(f\\\"./results_from_vm/{log_name}\\\"):\\n        os.makedirs(f\\\"./results_from_vm/{log_name}\\\")\\n\\n    # save data\\n    data_all.to_csv(\\n        f\\\"./results_from_vm/{log_name}/{data_type}_{log_name}_all.csv\\\",\\n        sep=\\\";\\\",\\n        index=False,\\n    )\\n\\n    # Display head and shape of the data\\n    # print(f\\\"Shape of {data_type} data:\\\")\\n    # print(data_all.shape())\\n    print(f\\\"Shape of {data_type} data: {data_all.shape}\\\\n\\\")\\n\\n    return data_all\\n\\n\\nlog_name = \\\"bpic2017\\\"  # \\\"bpic2012\\\"\\nsample_nr = 0\\n# Example usage for test and valid datasets\\ntest_data = read_and_preprocess_data(\\\"test\\\", sample_nr, log_name)\\nvalid_data = read_and_preprocess_data(\\\"valid\\\", sample_nr, log_name)\\n\\n\\ncalculate_statistics(test_data, \\\"test\\\")\\ncalculate_statistics(valid_data, \\\"valid\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def custom_encode(value):\n",
    "    if value == '[0]':\n",
    "        return 0\n",
    "    elif value in ('[]', '[1]'): # high uncertainty: empty set or set with positive outcome\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def encode_conformal_data(data):\n",
    "    # Apply the encoding function to columns that start with an alphabetical character\n",
    "    columns_to_encode = [col for col in data.columns if col.startswith(('alpha'))]\n",
    "    data[columns_to_encode] = data[columns_to_encode].applymap(custom_encode)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Function to read and preprocess data\n",
    "def read_and_preprocess_data(data_type, sample_nr=0, log_name=\"bpic2012\"):\n",
    "    # Read CSV files\n",
    "    data_csv = pd.read_csv(f\"./prepared_data/{log_name}/{data_type}_{log_name}.csv\", sep=';')[\n",
    "        [case_id_col, activity_col, timestamp_col, resource_col, label_col, \"Treatment1\"]\n",
    "    ]\n",
    "\n",
    "    data_encoded = pd.read_csv(f\"./prepared_data/{log_name}/{data_type}_encoded_{log_name}.csv\", sep=\";\")\n",
    "\n",
    "    bpic2012_sample = pd.read_csv(f\"./realcause_datasets_{log_name}/{log_name}_sample{sample_nr}.csv\")\n",
    "\n",
    "    common_columns = data_encoded.columns.intersection(bpic2012_sample.columns)\n",
    "    merged_df = pd.merge(data_encoded, bpic2012_sample, on=list(common_columns)).iloc[:, -5:]\n",
    "\n",
    "\n",
    "    # Read predictive + preds conformal\n",
    "    data_preds_conformal = pd.read_csv(f\"./results/conformal/{log_name}/conformal_{data_type}_{log_name}.csv\", sep=\";\")\n",
    "\n",
    "    # Read causal + conformal_causal\n",
    "    data_conformal_causal = pd.read_csv(f\"./results/conformal_causal/{log_name}/conformalizedTE_{log_name}_1_{data_type}.csv\", sep=\",\").iloc[:, -24:]\n",
    "\n",
    "    # Read Survival\n",
    "    data_survival = pd.read_csv(f\"./results/survival/{log_name}/survival_{data_type}_{log_name}.csv\", sep=\";\").iloc[:, -27:]\n",
    "\n",
    "    data_csv.reset_index(drop=True, inplace=True)\n",
    "    data_preds_conformal.reset_index(drop=True, inplace=True)\n",
    "    data_conformal_causal.reset_index(drop=True, inplace=True)\n",
    "    data_survival.reset_index(drop=True, inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_all = pd.concat([data_csv, data_preds_conformal, data_conformal_causal, data_survival, merged_df], axis=1)\n",
    "    data_all = data_all.dropna()\n",
    "\n",
    "    # Encode conformal data\n",
    "    data_all = encode_conformal_data(data_all)\n",
    "    data_all = assign_causal_class_labels(data_all)\n",
    "\n",
    "    sorting_cols = [timestamp_col]\n",
    "    data_all = data_all.sort_values(by=sorting_cols).reset_index(drop=True)\n",
    "\n",
    "    # cheack of results_from_vm folder exists\n",
    "    if not os.path.exists(f\"./results{log_name}\"):\n",
    "        os.makedirs(f\"./results/{log_name}\")\n",
    "\n",
    "    # save data\n",
    "    data_all.to_csv(f\"./results/{log_name}/{data_type}_{log_name}_all.csv\", sep=\";\", index=False)\n",
    "\n",
    "\n",
    "    return data_all\n",
    "\n",
    "logs = [\"bpic2012\", \"bpic2017\"]\n",
    "for log_name in logs:\n",
    "    sample_nr = 0\n",
    "    test_data = read_and_preprocess_data(\"test\", sample_nr, log_name)\n",
    "    valid_data = read_and_preprocess_data(\"valid\", sample_nr, log_name)\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
