{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# standardize column names: [case_id_col, 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = \"case_id\"\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def custom_encode(value):\n",
    "    if value == '[0]':\n",
    "        return 0\n",
    "    elif value in ('[]', '[1]', \"[1,0]\", \"[0,1]\"): # high uncertainty\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def encode_conformal_data(data):\n",
    "    # Apply the encoding function to columns that start with an alphabetical character\n",
    "    columns_to_encode = [col for col in data.columns if col.startswith(('alpha'))]\n",
    "    data[columns_to_encode] = data[columns_to_encode].applymap(custom_encode)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Function to read and preprocess data\n",
    "def read_and_preprocess_data(data_type, sample_nr=0, log_name=\"bpic2012\"):\n",
    "    # Read CSV files\n",
    "    data_csv = pd.read_csv(f\"./prepared_data/{log_name}/{data_type}_{log_name}.csv\", sep=';')[\n",
    "        [case_id_col, activity_col, timestamp_col, resource_col, label_col, \"Treatment1\"]\n",
    "    ]\n",
    "\n",
    "    data_encoded = pd.read_csv(f\"./prepared_data/{log_name}/{data_type}_encoded_{log_name}.csv\", sep=\";\")\n",
    "\n",
    "    bpic2012_sample = pd.read_csv(f\"./realcause_datasets_{log_name}/{log_name}_sample{sample_nr}.csv\")\n",
    "\n",
    "    common_columns = data_encoded.columns.intersection(bpic2012_sample.columns)\n",
    "    merged_df = pd.merge(data_encoded, bpic2012_sample, on=list(common_columns)).iloc[:, -5:]\n",
    "\n",
    "\n",
    "    # Read predictive + preds conformal\n",
    "    data_preds_conformal = pd.read_csv(f\"./results/conformal/{log_name}/conformal_{data_type}_{log_name}.csv\", sep=\";\")\n",
    "\n",
    "    # Read causal + conformal_causal\n",
    "    data_conformal_causal = pd.read_csv(f\"./results/conformal_causal/{log_name}/conformalizedTE_{log_name}_1_{data_type}.csv\", sep=\",\").iloc[:, -24:]\n",
    "\n",
    "    # Read Survival\n",
    "    data_survival = pd.read_csv(f\"./results/survival/{log_name}/survival_{data_type}_{log_name}.csv\", sep=\";\").iloc[:, -27:]\n",
    "\n",
    "    data_csv.reset_index(drop=True, inplace=True)\n",
    "    data_preds_conformal.reset_index(drop=True, inplace=True)\n",
    "    data_conformal_causal.reset_index(drop=True, inplace=True)\n",
    "    data_survival.reset_index(drop=True, inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_all = pd.concat([data_csv, data_preds_conformal, data_conformal_causal, data_survival, merged_df], axis=1)\n",
    "    data_all = data_all.dropna()\n",
    "\n",
    "    # Encode conformal data\n",
    "    data_all = encode_conformal_data(data_all)\n",
    "    data_all = assign_causal_class_labels(data_all)\n",
    "\n",
    "    sorting_cols = [timestamp_col]\n",
    "    data_all = data_all.sort_values(by=sorting_cols).reset_index(drop=True)\n",
    "\n",
    "    # cheack of results_from_vm folder exists\n",
    "    if not os.path.exists(f\"./results{log_name}\"):\n",
    "        os.makedirs(f\"./results/{log_name}\")\n",
    "\n",
    "    # save data\n",
    "    data_all.to_csv(f\"./results/{log_name}/{data_type}_{log_name}_all.csv\", sep=\";\", index=False)\n",
    "\n",
    "\n",
    "    return data_all\n",
    "\n",
    "logs = [\"bpic2012\", \"bpic2017\"]\n",
    "for log_name in logs:\n",
    "    sample_nr = 0\n",
    "    test_data = read_and_preprocess_data(\"test\", sample_nr, log_name)\n",
    "    valid_data = read_and_preprocess_data(\"valid\", sample_nr, log_name)\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
