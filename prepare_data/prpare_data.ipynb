{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install modin[all]\n",
    "# !pip install bokeh==2.4.2\n",
    "# !pip install psutil memory-profiler\n",
    "# !pip install jupyter-server-proxy\n",
    "# !pip install pm4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs = [\n",
    "        'bpic2012',\n",
    "        'bpic2017', \n",
    "        #'trafficFines',\n",
    "        ]\n",
    "\n",
    "\n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "\n",
    "# dataset_name: [case_id_col, activity_col, resource_col, timestamp_col]\n",
    "# NOTE - Basic column names are defined by users. These names will be renamed and standardized later \n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp']\n",
    "dataset_dict = {\n",
    "    'bpic2012': ['case_id', 'activity', 'resource', 'start_time'],\n",
    "    'bpic2017': ['case_id', 'activity', 'org:resource', 'time:timestamp'],\n",
    "    #'trafficFines': ['case:concept:name', 'concept:name', 'org:resource', \"time:timestamp\"], \n",
    "}\n",
    "incomplete_dict = {\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"],\n",
    "    'bpic2017': [\"A_Pending\", \"A_Denied\", \"A_Cancelled\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Define positive and negative labels\n",
    "label_old = \"label\"\n",
    "neg_label = \"deviant\"\n",
    "pos_label = \"regular\"\n",
    "positive_activities_dict = {\n",
    "    'bpic2017': [\"A_Pending\"],\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "chunk_s = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "from pandas import Timestamp\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "class DataPreprocessorModinDask:\n",
    "    def __init__(self, log_name, input_data_folder, output_data_folder):\n",
    "        self.log_name = log_name\n",
    "        self.input_data_folder = input_data_folder\n",
    "        self.output_data_folder = output_data_folder\n",
    "\n",
    "    def _print_metrics(self, elapsed_time):\n",
    "        # Helper function to print resource usage metrics\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        # print(f\"Time: {elapsed_time} seconds\")\n",
    "        # print(f\"CPU Usage: {cpu_percent}%\")\n",
    "        # print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "    @profile\n",
    "    def read_log(self):\n",
    "        start_time = time.time()\n",
    "        print(\"Reading log...\")\n",
    "\n",
    "        # Get the first file in the input folder\n",
    "        files = os.listdir(self.input_data_folder)[0]\n",
    "        log_path = os.path.join(self.input_data_folder, files)\n",
    "\n",
    "        # Read the log based on its extension\n",
    "        if log_path.lower().endswith('.csv'):\n",
    "            with open(log_path, 'r') as file:\n",
    "                # Use Sniffer to infer the delimiter\n",
    "                dialect = csv.Sniffer().sniff(file.read(10000))\n",
    "            try:\n",
    "                # Read CSV with Dask, specifying dtype for certain columns\n",
    "                log_file = pd.read_csv(log_path, sep=dialect.delimiter, dtype={'Resource': 'object', 'article': 'object'})\n",
    "            except:\n",
    "                log_file = pd.read_csv(log_path, sep=dialect.delimiter)\n",
    "            log_file = log_file.rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "            log_file = log_file.rename(\n",
    "                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\n",
    "        elif log_path.lower().endswith('.xes') or log_path.lower().endswith('.xes.gz'):\n",
    "            # Read XES log using pm4py and convert to Dask DataFrame\n",
    "            log_file = pm4py.read_xes(log_path)\n",
    "            log_file = dataframe_utils.convert_traces_dataframe(log_file)\n",
    "            log_file = from_pandas(log_file, npartitions=1)\n",
    "            log_file = log_file.rename(\n",
    "                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file extension. Supported extensions: .csv, .xes\")\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "\n",
    "    @profile\n",
    "    def clean_data(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Cleaning data...\")\n",
    "\n",
    "        # Convert timestamp column to datetime\n",
    "        log_file[timestamp_col] = pd.to_datetime(log_file[timestamp_col], format=\"mixed\", infer_datetime_format=True)\n",
    "        log_file = log_file.sort_values(by=[timestamp_col])\n",
    "\n",
    "       # Remove white spaces from column values\n",
    "        log_file = log_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        # Convert the 'resource' column to string\n",
    "        log_file[resource_col] = log_file[resource_col].astype(str)\n",
    "\n",
    "        # Replace unique resource values with 'res{i}' format\n",
    "        unique_resources = log_file[resource_col].unique()\n",
    "        resource_mapping = {original_value: f'res{i + 1}' for i, original_value in enumerate(unique_resources)}\n",
    "        log_file[resource_col] = log_file[resource_col].replace(resource_mapping)\n",
    "\n",
    "        threshold_percentage = 25\n",
    "        threshold = len(log_file) * (threshold_percentage / 100)\n",
    "        columns_to_drop = log_file.columns[log_file.isna().sum() > threshold].tolist()\n",
    "        log_file = log_file.drop(columns=columns_to_drop)\n",
    "\n",
    "        log_file = log_file.dropna()\n",
    "\n",
    "        activities_to_check = incomplete_dict[log_name]  # [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"]       \n",
    "        contains_activity = log_file[activity_col].isin(activities_to_check).groupby(log_file[case_id_col]).max().reset_index()\n",
    "        complete_cases = contains_activity[contains_activity[activity_col] == True][case_id_col]\n",
    "        log_file = log_file[log_file[case_id_col].isin(complete_cases.tolist())]       \n",
    "\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "\n",
    "    \n",
    "    @profile\n",
    "    def extract_temporal_features(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Extracting timestamp features...\")\n",
    "\n",
    "\n",
    "        # Calculate event_nr\n",
    "        log_file['event_nr'] = log_file.groupby(case_id_col).cumcount() + 1\n",
    "\n",
    "        # Calculate case_length\n",
    "        log_file['case_length'] = log_file.groupby(case_id_col)['event_nr'].transform('max')\n",
    "\n",
    "        # Extract temporal context information\n",
    "        log_file['hour_of_day'] = log_file[timestamp_col].dt.hour\n",
    "        log_file['day_of_week'] = log_file[timestamp_col].dt.dayofweek + 1  # Monday is 1, Sunday is 7\n",
    "        log_file['day_of_month'] = log_file[timestamp_col].dt.day\n",
    "        log_file['month_of_year'] = log_file[timestamp_col].dt.month\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "\n",
    "\n",
    "        return log_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "results_data = []\n",
    "# Initialize your DataPreprocessorModinDask object and call read_log() and clean_data()\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    input_data_folder = \"/home/mshoush/4thyearV2/code/PrPM_framework/data/%s\" % (log_name,)\n",
    "    output_data_folder = \"/home/mshoush/4thyearV2/code/PrPM_framework/prepared_data/%s\" % log_name\n",
    "\n",
    "    data_preprocessor = DataPreprocessorModinDask(log_name, input_data_folder, output_data_folder)\n",
    "    log_file = data_preprocessor.read_log()\n",
    "    cleaned_data = data_preprocessor.clean_data(log_file)\n",
    "    features_data = data_preprocessor.extract_temporal_features(cleaned_data)\n",
    "    features_data.name = \"prepared_features_data_%s.csv\" % log_name\n",
    "    results_data.append(features_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    features_data.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, features_data.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Interventions For BPIC2017 from the winner student report:\n",
    "\n",
    "1. **Sending Another Loan Offer:**\n",
    "   - *Intervention:* Send offers to clients as soon as possible. For all case endpoints, this has been shown to have the greatest effect on cancellation rates. Sending offers to clients within 4 days may decrease cancellation rates by 5% up to 10%.\n",
    "   - *Treatment 1:* Cases that receive only one offer are in the control group, while cases that receive more than one offer are in the treatment group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "results_data2 = []\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    positive_activities = positive_activities_dict[log_name]\n",
    "\n",
    "    # Initialize a global state to keep track of positive activities across cases\n",
    "    global_positive_cases = set()\n",
    "\n",
    "    # Define a function to label each group\n",
    "    def label_group(chunk):\n",
    "        # Check if any positive activity exists in the case\n",
    "        chunk['is_positive'] = chunk.groupby('case_id')[activity_col].transform(lambda x: any(activity in positive_activities for activity in x))\n",
    "        \n",
    "        # Update the global positive cases state\n",
    "        positive_cases_from_chunk = set(chunk.loc[chunk['is_positive'], 'case_id'])\n",
    "        global_positive_cases.update(positive_cases_from_chunk)\n",
    "        \n",
    "        # Assign label based on the result\n",
    "        chunk['label'] = chunk.apply(lambda row: pos_label if row['is_positive'] else neg_label, axis=1)\n",
    "        \n",
    "        return chunk.drop('is_positive', axis=1)\n",
    "\n",
    "    # Set the chunk size based on your system's memory constraints\n",
    "    chunk_size = chunk_s\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    features_data = results_data[logs.index(log_name)]\n",
    "\n",
    "    num_chunks = len(features_data) // chunk_size\n",
    "    chunks = [features_data.iloc[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks + 1)]\n",
    "\n",
    "    # Initialize a ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(label_group, chunk) for chunk in chunks]\n",
    "\n",
    "        # Collect the results as they become available\n",
    "        results = []\n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Concatenate the results of all chunks into a final DataFrame\n",
    "    labeled_data = pd.concat(results, ignore_index=True)\n",
    "    results_data2.append(labeled_data)\n",
    "    \n",
    "\n",
    "    labeled_data.name = \"labeled_data_%s.csv\" % log_name\n",
    "\n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    labeled_data.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, labeled_data.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "results_data3 = []\n",
    "\n",
    "relevant_activities_dict = {\n",
    "    \"bpic2017\": ['O_Sent (mail and online)', 'O_Sent (online only)'],\n",
    "    \"bpic2012\": ['O_SENT'],\n",
    "    \"trafficFines\": ['Add penalty']\n",
    "    }\n",
    "\n",
    "# Function for Treatment 1: Increase the number of offers\n",
    "def apply_treatment1(group, log_name):\n",
    "\n",
    "    treatment2_col = \"numberofterms\"\n",
    "    treatment3_col = \"firstwithdrawalamount\"\n",
    "    treatment4_col = \"monthlycost\"\n",
    "\n",
    "\n",
    "    \n",
    "    if log_name == \"bpic2017\":        \n",
    "        # Count how much offers sent in mail and online versus online only within each group\n",
    "        group['Mail_and_Online_Count'] = (group[activity_col] == 'O_Sent (mail and online)').sum()\n",
    "        group['Online_Only_Count'] = (group[activity_col] == 'O_Sent (online only)').sum()\n",
    "        group['Total_Offers'] = group['Mail_and_Online_Count'] + group['Online_Only_Count']\n",
    "\n",
    "        # Determine the treatment or control based on the counts within each group\n",
    "        group['Treatment1'] = 'Control'\n",
    "        if group['Mail_and_Online_Count'].sum() > 1 or group['Online_Only_Count'].sum() > 1:\n",
    "            group['Treatment1'] = 'Treatment'\n",
    "\n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        return group\n",
    "\n",
    "    elif log_name == \"bpic2012\":\n",
    "        group['Total_Offers'] = (group[activity_col] == 'O_SENT').sum()\n",
    "        group['Treatment1'] = 'Control'\n",
    "        if group['Total_Offers'].sum() > 1:\n",
    "            group['Treatment1'] = 'Treatment'\n",
    "        \n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "        return group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "treatments_functions = [apply_treatment1,] # apply_treatment2, apply_treatment3, apply_treatment4]\n",
    "\n",
    "# Set the maximum number of cases per chunk\n",
    "max_cases_per_chunk = 100\n",
    "\n",
    "\n",
    "# List to store futures\n",
    "futures = []\n",
    "\n",
    "# Set the maximum number of worker processes\n",
    "max_workers = 7  # You can adjust this number based on your system's capabilities\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_time_to_last_event_column(df, case_id_col, timestamp_col, activity_col, activities_to_track):\n",
    "    \"\"\"\n",
    "    Add a new column representing the time (in days) to the last occurrence of specified activities for each case.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the event log.\n",
    "    - case_id_col (str): The column name representing case IDs.\n",
    "    - timestamp_col (str): The column name representing timestamps.\n",
    "    - activity_col (str): The column name representing activity names.\n",
    "    - activities_to_track (list): List of activity names to track the time to the last occurrence.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    # Ensure timestamps are in datetime format\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # Filter for specified activities\n",
    "    filtered_df = df[df[activity_col].isin(activities_to_track)]\n",
    "\n",
    "    # Find the last occurrence for each case\n",
    "    last_occurrences = filtered_df.groupby(case_id_col)[timestamp_col].max()\n",
    "\n",
    "    # Merge the last occurrences back to the original DataFrame\n",
    "    df = pd.merge(df, last_occurrences, how='left', on=case_id_col, suffixes=('', '_last'))\n",
    "\n",
    "    # Calculate the time difference in days\n",
    "    df['time_to_last_event_days'] = (df['timestamp_last'] - df[timestamp_col]).dt.total_seconds() / (60 * 60 * 24)\n",
    "\n",
    "    # Fill NaN values (cases without the specified activities) with a default value, e.g., -1\n",
    "    df['time_to_last_event_days'].fillna(-1, inplace=True)\n",
    "\n",
    "    # Drop the auxiliary columns used for the calculation\n",
    "    df.drop(['timestamp_last'], axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def determine_overall_treatment(row):\n",
    "    treatments = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\n",
    "    for treatment in treatments:\n",
    "        if row[treatment] == 'Treatment':\n",
    "            return treatment\n",
    "    return 'Controle'\n",
    "\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))    \n",
    "    relevant_activities = relevant_activities_dict[log_name]\n",
    "    labeled_data = results_data2[logs.index(log_name)]\n",
    "  \n",
    "    futures = []\n",
    "    results = []\n",
    "        \n",
    "    # Initialize a ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for treatment_function in treatments_functions:\n",
    "            # Split the DataFrame into chunks based on case_id_col\n",
    "            grouped = None\n",
    "            grouped = labeled_data.groupby(case_id_col, as_index=False)\n",
    "            # Iterate over groups and submit tasks to the executor\n",
    "            for name, group in tqdm(grouped, desc=\"Submitting Tasks\"):\n",
    "                # Reset the index for each group\n",
    "                group.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "                # Submit the task to the executor for each group\n",
    "                future = executor.submit(treatment_function, group.copy(), log_name,)\n",
    "                futures.append(future)\n",
    "\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Collecting Results\"):\n",
    "        try:\n",
    "            treated_chunk = future.result()\n",
    "            results.append(treated_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting result for {future}: {e}\")\n",
    "\n",
    "    try:\n",
    "        final_result = pd.concat(results, ignore_index=True)\n",
    "        final_result.reset_index(drop=True, inplace=True)  # Resetting index to avoid duplicate indices\n",
    "        # print(\"Successfully concatenated results into final DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating results into final DataFrame: {e}\")\n",
    "\n",
    "    \n",
    "    # add time to last event column\n",
    "    activities_to_track = incomplete_dict[log_name]\n",
    "    #print(\"Activities to track:\", activities_to_track)\n",
    "    final_result = add_time_to_last_event_column(final_result, case_id_col, timestamp_col, activity_col, activities_to_track)\n",
    "\n",
    "    # Display the final result\n",
    "    print(\"\\nDone! - Final DataFrame:\\n\")\n",
    "    final_result.name = \"date_with_treatments_%s.csv\" % log_name\n",
    "    #print(\"shape after treatment:\", final_result.shape[0], \"rows\", final_result.shape[1], \"columns\")\n",
    "\n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    results_data3.append(final_result)\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    final_result.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, final_result.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "    final_result = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define the label column\n",
    "label_col = 'label'\n",
    "\n",
    "\n",
    "def split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22):\n",
    "    # Split data into train, val, and test sets based on the specified ratios\n",
    "    grouped = data.groupby(case_id_col)\n",
    "    start_timestamps = grouped[timestamp_col].min().reset_index()\n",
    "\n",
    "    # Sort start_timestamps based on the split_type\n",
    "    if split_type == \"temporal\":\n",
    "        start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "    elif split_type == \"random\":\n",
    "        np.random.seed(seed)\n",
    "        start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "\n",
    "    train_size = int(train_ratio * len(start_timestamps))\n",
    "    val_size = int(val_ratio * len(start_timestamps))\n",
    "    test_size = len(start_timestamps) - train_size - val_size\n",
    "\n",
    "    train_ids = list(start_timestamps[case_id_col])[:train_size]\n",
    "    val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\n",
    "    test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\n",
    "\n",
    "    train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "    val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "    test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def encode_data(data, treatment_cols, label_col, data_type):\n",
    "    y_numeric = np.where(data[label_col] == 'deviant', 0, 1)\n",
    "\n",
    "    T_numeric = [np.where(data[treatment] == \"Treatment\", 1, 0) for treatment in treatment_cols]\n",
    "\n",
    "    numeric_cols = data.select_dtypes(include='number').columns\n",
    "    # Exclude label_col and treatment_cols from categorical_cols\n",
    "    exclude_cols = [label_col] + treatment_cols\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns.difference(exclude_cols)\n",
    "\n",
    "\n",
    "    # Convert numeric columns to float\n",
    "    data[numeric_cols] = data[numeric_cols].astype(float)\n",
    "\n",
    "    # Convert categorical columns to categorical type\n",
    "    data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "\n",
    "    # Apply cat.codes only to categorical columns\n",
    "    data[categorical_cols] = data[categorical_cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    data_encoded = pd.concat([\n",
    "        pd.DataFrame(data[numeric_cols]),  # Keep numeric columns\n",
    "        pd.DataFrame(y_numeric, columns=['Outcome']),\n",
    "        pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\n",
    "        pd.DataFrame(data[categorical_cols], columns=categorical_cols),\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "    return data_encoded\n",
    "\n",
    "def save_data(data, results_dir, dataset_name, data_type):\n",
    "    data.to_csv(os.path.join(results_dir, f\"{data_type}_{dataset_name}.csv\"), sep=\";\", index=False)\n",
    "\n",
    "def save_data_encoded(data_encoded, results_dir, dataset_name, data_type):\n",
    "    data_encoded.to_csv(os.path.join(results_dir, f\"{data_type}_encoded_{dataset_name}.csv\"), sep=\";\", index=False)\n",
    "\n",
    "\n",
    "sorting_cols = [timestamp_col, activity_col]\n",
    "\n",
    "\n",
    "# Specify the desired ratios\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.3\n",
    "test_ratio = 0.2\n",
    "\n",
    "\n",
    "\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    treatments_data = results_data3[logs.index(log_name)]\n",
    "\n",
    "    data = treatments_data.copy()\n",
    "    all_columns = data.columns\n",
    "\n",
    "    # Split the data into train, val, and test\n",
    "    train, val, test = split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22)\n",
    "\n",
    "   \n",
    "    treatment_cols = ['Treatment1',]\n",
    "\n",
    "\n",
    "    save_data(train, results_dir, log_name, \"train\")\n",
    "    save_data(test, results_dir, log_name, \"test\")\n",
    "    save_data(val, results_dir, log_name, \"valid\")\n",
    "\n",
    "\n",
    "    train_data_encoded = encode_data(train, treatment_cols, label_col, \"train\")\n",
    "    test_data_encoded = encode_data(test, treatment_cols, label_col, \"test\")\n",
    "    val_data_encoded = encode_data(val, treatment_cols, label_col, \"valid\")\n",
    "\n",
    "    #dataset_name = log_name\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    \n",
    "    print(\"Saving the train, val, and test sets to separate files...\")\n",
    "    save_data_encoded(train_data_encoded, results_dir, log_name, \"train\")\n",
    "    save_data_encoded(test_data_encoded, results_dir, log_name, \"test\")\n",
    "    save_data_encoded(val_data_encoded, results_dir, log_name, \"valid\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/train_encoded_bpic2017.csv\", sep=\";\")\n",
    "test_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/test_encoded_bpic2017.csv\", sep=\";\")\n",
    "valid_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/valid_encoded_bpic2017.csv\", sep=\";\")\n",
    "bpic2017_encoded = pd.concat([train_encoded_17, test_encoded_17, valid_encoded_17], axis=0)\n",
    "# save the encoded data to separate files\n",
    "\n",
    "bpic2017_encoded.to_csv(\"./../prepared_data/bpic2017/bpic2017_encoded.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/train_encoded_bpic2012.csv\", sep=\";\")\n",
    "test_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/test_encoded_bpic2012.csv\", sep=\";\")\n",
    "valid_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/valid_encoded_bpic2012.csv\", sep=\";\")\n",
    "\n",
    "bpic2012_encoded = pd.concat([train_encoded_12, valid_encoded_12, test_encoded_12], axis=0)\n",
    "# save the encoded data to separate files\n",
    "\n",
    "bpic2012_encoded.to_csv(\"./../prepared_data/bpic2012/bpic2012_encoded.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
